{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBlyEIsLP+adQgDwjHLuq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaurabhSRP/04-NLP-based-Projects/blob/main/Next%20Word%20Prediction%20using%20LSTM/Next_Word_Prediction_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "IY2VEC_8eOlc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rt7JaymFVEZt"
      },
      "outputs": [],
      "source": [
        "file = open(\"/content/The_Whale.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines) ##Forms a single text file instead of list of lines or sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing and cleaning**"
      ],
      "metadata": {
        "id": "GjQ1niWZrkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data=re.sub(\"[^a-zA-Z]\",' ',data)\n",
        "data=data.lower()\n",
        "data=data.split()\n",
        "\n",
        "data=' '.join(data)\n",
        "corpus=data[10000:15000] #taking the corpus of 5000 words because google colab cant handle this big data"
      ],
      "metadata": {
        "id": "kS5fx52Fg3Eh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cssPJL3GbMO2",
        "outputId": "8f105e3c-6ff6-422a-fb7f-3c54bb227bdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**\n",
        "* Tokenization\n",
        "* Converting each word with a unique number"
      ],
      "metadata": {
        "id": "xkM2LVamrsm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() ##imported from tensorflow\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([corpus])[0] #converting each word/token to a sequence number\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeeBSsAhZ8H4",
        "outputId": "925d7a0a-b7d4-4a08-d54c-e06919b5c79c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 4, 8, 117, 16, 5, 118, 58, 119, 2, 120, 7, 59, 121, 122]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf21R2wxeWNK",
        "outputId": "2a50305f-92bb-4984-9b2a-c8c2dd7cbab0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "964"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12TpO7n2enxG",
        "outputId": "6c7c0df8-9b59-43bc-876b-a684d59f943b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation for LSTM**"
      ],
      "metadata": {
        "id": "McqEYvU9r7mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [] #Here we create a dataset such that previous 4 words will become X and 5th word will be the prediction\n",
        "\n",
        "for i in range(4, len(sequence_data)):\n",
        "    words = sequence_data[i-4:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5f0RKFPeqQE",
        "outputId": "0fc22d24-d51f-4518-9b95-9a7667a7f880"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  960\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 24,   4,   8, 117,  16],\n",
              "       [  4,   8, 117,  16,   5],\n",
              "       [  8, 117,  16,   5, 118],\n",
              "       [117,  16,   5, 118,  58],\n",
              "       [ 16,   5, 118,  58, 119],\n",
              "       [  5, 118,  58, 119,   2],\n",
              "       [118,  58, 119,   2, 120],\n",
              "       [ 58, 119,   2, 120,   7],\n",
              "       [119,   2, 120,   7,  59],\n",
              "       [  2, 120,   7,  59, 121]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:4])\n",
        "    y.append(i[4])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "J1sHzR-ketLG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ],
      "metadata": {
        "id": "BOeRYEjNewAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895c6538-5e0f-4935-e1f4-f79d36d6d650"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[ 24   4   8 117]\n",
            " [  4   8 117  16]\n",
            " [  8 117  16   5]\n",
            " [117  16   5 118]\n",
            " [ 16   5 118  58]\n",
            " [  5 118  58 119]\n",
            " [118  58 119   2]\n",
            " [ 58 119   2 120]\n",
            " [119   2 120   7]\n",
            " [  2 120   7  59]]\n",
            "Response:  [ 16   5 118  58 119   2 120   7  59 121]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5] \n",
        "##Creating a matrix to match the vocab size ,same as padding"
      ],
      "metadata": {
        "id": "HM5Mtn9qeyLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4bdeb3-fd10-431c-f512-9e42330f42c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Creation**"
      ],
      "metadata": {
        "id": "jC1c7NY1sA4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=4)) #word embedding\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "Yu_VHuF0lWSV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or_wihSUnfct",
        "outputId": "88144425-0e13-479b-e2f7-054942649d5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 10)             4660      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 4, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 466)               466466    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,520,126\n",
            "Trainable params: 13,520,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Visualisation**"
      ],
      "metadata": {
        "id": "rRPgn8lnn2od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "OkpZE-WsnfwB",
        "outputId": "5a96e09a-dd19-4bc4-b879-bd33e0d985f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAIjCAYAAADC5+TxAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3de1RU570+8GfPBeaiDAgISQAVjCJekqPRGKLGaM3RJLVRUFHx1trj5eRYY1SS6LGutiamaqBNNVlGj805zcJBSDWml2NPNMRETY0hmqh4I96KCCqCMigDfn9/5Me0U26DvMwM8nzWmj945539fvfeMw/7MrO3JiICIiIFdL4ugIjuHQwUIlKGgUJEyjBQiEgZwz837N+/H2+88YYvaiGiNmTRokV47LHH3NrqbKFcuHAB2dnZXiuKqNaBAwdw4MABX5dBHsjOzsaFCxfqtNfZQqm1bdu2Vi2I6J9NmDABAN97bYGmafW28xgKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlLGrwNl4MCB0Ov1ePjhh5VPe/bs2ejYsSM0TcNXX33V7H5//OMfYbPZsHPnTuW1NZc/1eJNBw4cQK9evaDT6aBpGiIiIvCLX/zC12W5ycnJQWxsLDRNg6ZpiIyMRGpqqq/LajV+HSgHDx7Ek08+2SrT3rRpE95555277udPdx/xp1q8afDgwTh+/DieeuopAMCJEyewfPlyH1flLikpCQUFBYiLi4PNZkNRURF+97vf+bqsVtPgBZb8SUMXc/GlZ555BmVlZb4uA4B/1VJZWYmRI0di3759vi7FJ9r7/Pv1Fkoto9HYKtP1NKi8EWgigm3btmHjxo2tPlZr2rx5M4qLi31dhs+09/lXEig1NTVYsWIFYmJiYDab0a9fP9jtdgBARkYGrFYrdDodBgwYgIiICBiNRlitVvTv3x9Dhw5FdHQ0TCYTgoODsXTp0jrTP336NOLj42G1WmE2mzF06FB8+umnHtcAfPeBXbNmDXr27InAwEDYbDYsWbKkzlie9Pv0008RExMDTdPwm9/8BgCwYcMGWK1WWCwW7NixA2PGjEFQUBCioqKQmZlZp9ZXX30VPXv2hNlsRlhYGLp164ZXX30VEydObNayb0ktv/71r2EymdC5c2fMnTsX9913H0wmExITE/H555+7+i1YsAABAQGIjIx0tf37v/87rFYrNE3DlStXAAALFy7Eiy++iDNnzkDTNHTv3r1Z86JKW5//vXv3IiEhATabDSaTCX379sX//u//AvjumF7t8Zi4uDjk5eUBAGbNmgWLxQKbzYYPPvgAQOOfiV/+8pewWCzo2LEjiouL8eKLL+KBBx7AiRMn7qpmF/kndrtd6mlu1OLFiyUwMFCys7OltLRUXnnlFdHpdHLw4EEREfnpT38qAOTzzz+XiooKuXLliowePVoAyB/+8AcpKSmRiooKWbBggQCQr776yjXtkSNHSmxsrHz77bfidDrlm2++kUcffVRMJpOcPHnS4xqWLVsmmqbJunXrpLS0VBwOh6xfv14ASF5enms6nva7cOGCAJA333zT7bUA5KOPPpKysjIpLi6WoUOHitVqlaqqKle/VatWiV6vlx07dojD4ZBDhw5JRESEDB8+vFnLXUUtc+bMEavVKseOHZNbt27J0aNHZeDAgdKxY0c5f/68q9/UqVMlIiLCbdw1a9YIACkpKXG1JSUlSVxc3F3NR3JysiQnJzf7df/6r/8qAKS0tNTV5m/zHxcXJzabzaP52bZtm6xcuVKuXbsmV69elcGDB0toaKjbGHq9Xv72t7+5vW7KlCnywQcfuP725DMBQH7yk5/Im2++KePHj5fjx497VCMAsdvtddpbvIVy69YtbNiwAePGjUNSUhKCg4OxfPlyGI1GbNmyxa1vQkICLBYLQkNDMXnyZABATEwMwsLCYLFYXEe/8/Pz3V7XsWNHdO3aFQaDAb1798Y777yDW7duuXYPmqqhsrIS6enp+N73vodFixYhODgYZrMZnTp1chvH035NSUxMRFBQEMLDw5GSkoKKigqcP3/e9fz27dsxYMAAjB07FmazGf3798cPfvADfPLJJ6iqqmrWWC2tBQAMBgN69eqFwMBAJCQkYMOGDbhx40ad9dcWtcX5T05Oxk9/+lOEhISgU6dOGDt2LK5evYqSkhIAwLx581BTU+NWX3l5OQ4ePIinn34aQPM+l6tXr8bzzz+PnJwcxMfHt6j2FgfKiRMn4HA40KdPH1eb2WxGZGRknWD4RwEBAQCA6upqV1vtsRKn09nomH379oXNZsORI0c8quH06dNwOBwYOXJko9P1tF9z1M7nP87TrVu36pyZqampgdFohF6vVza2J7XU55FHHoHFYml0/bVFbXX+az8XNTU1AIARI0agR48e+K//+i/X+2jr1q1ISUlxvX/u9nPZUi0OlIqKCgDA8uXLXft2mqbh3LlzcDgcLS6wIUaj0fXGaKqGixcvAgDCw8Mbnaan/Vrq6aefxqFDh7Bjxw5UVlbiiy++wPbt2/Hss8+2aqA0R2BgoOs/Ynvky/n/wx/+gOHDhyM8PByBgYF1jitqmoa5c+eioKAAH330EQDgv//7v/GjH/3I1cdXn8sWB0rthy89PR0i4vbYv39/iwusT3V1Na5du4aYmBiPajCZTACA27dvNzpdT/u11MqVKzFixAjMnDkTQUFBGD9+PCZOnOjR92K8wel04vr164iKivJ1KT7h7fn/5JNPkJ6eDgA4f/48xo0bh8jISHz++ecoKyvD66+/Xuc1M2fOhMlkwqZNm3DixAkEBQWhS5curud98bkEFHwPpfYMTWPfNlVtz549uHPnDvr37+9RDX369IFOp0Nubi7mzZvX4HQ97ddSR48exZkzZ1BSUgKDwf++CvTxxx9DRDB48GBXm8FgaHJX4V7h7fk/dOgQrFYrAODrr7+G0+nE/PnzERsbC6D+ry2EhIRg0qRJ2Lp1Kzp27Igf//jHbs/74nMJKNhCMZlMmDVrFjIzM7FhwwaUl5ejpqYGFy9exKVLl1TUiKqqKpSVlaG6uhpffvklFixYgC5dumDmzJke1RAeHo6kpCRkZ2dj8+bNKC8vx5EjR+p858PTfi31/PPPIyYmBjdv3lQ63bt1584dlJaWorq6GkeOHMHChQsRExPjWr4A0L17d1y7dg3bt2+H0+lESUkJzp07V2danTp1QmFhIc6ePYsbN260iRDy1fw7nU5cvnwZH3/8sStQare6/+///g+3bt3CqVOn3E5h/6N58+bh9u3b+PDDD/H973/f7TlvfC7r9c+nfe7mtPHt27clLS1NYmJixGAwSHh4uCQlJcnRo0clIyNDLBaLAJCuXbvK3r17ZfXq1WKz2QSAREREyHvvvSdbt26ViIgIASAhISGSmZkpIiJbtmyRJ598Ujp37iwGg0FCQ0Nl8uTJcu7cOY9rEBG5ceOGzJ49W0JDQ6VDhw4yZMgQWbFihQCQqKgoOXz4sMf93nzzTYmMjBQAYrFYZOzYsbJ+/XrXfD744INy5swZ2bhxowQFBQkA6dKli+s09+7duyU0NFQAuB5Go1F69eolOTk5zVr2La1lzpw5YjQa5YEHHhCDwSBBQUHy3HPPyZkzZ9zGuXr1qjz55JNiMpmkW7du8h//8R+yZMkSASDdu3d3nWL98ssvpUuXLmI2m2XIkCFSVFTk8bw097TxgQMHpHfv3qLT6QSAREZGyqpVq/xq/t966y2Ji4tzW9f1Pd5//33XWGlpadKpUycJDg6WCRMmyG9+8xsBIHFxcW6nskVE/uVf/kVefvnlepdPY5+J119/XcxmswCQ6Oho+Z//+R+Pl7tIw6eNlQQKNc/69etl4cKFbm23b9+WF154QQIDA8XhcHitljlz5kinTp28Nl5j7vZ7KC3hT/N/N55++mkpKCjw+rgNBYr/7cDf44qKirBgwYI6+7YBAQGIiYmB0+mE0+mE2Wz2Wk21pyPbq7Y0/06n03Ua+ciRIzCZTOjWrZuPq/q7NvFbnnuJ2WyG0WjE5s2bcfnyZTidThQWFmLTpk1YsWIFUlJSUFhY6Haqr6FHSkqKr2eHvCwtLQ2nTp3CyZMnMWvWLPz85z/3dUluGCheZrPZsGvXLnzzzTfo0aMHzGYzEhISsGXLFqxevRrvvvsu4uPj65zqq++xdevWFtXyyiuvYMuWLSgrK0O3bt2QnZ2taC7bhrY4/xaLBfHx8fje976HlStXIiEhwdcludH+//6QS1ZWFiZNmtRur7FBvjNhwgQAwLZt23xcCTVF0zTY7fY6P2blFgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDV5gqfaXn0TecuDAAQB877VldQIlOjoaycnJvqiF/NQXX3wB4LsbYLWmf7zKPPm35ORkREdH12mvcz0Uon9We82LrKwsH1dC/o7HUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBlNRMTXRZD/+O1vf4uMjAzU1NS42kpKSgAA4eHhrja9Xo+FCxdi5syZ3i6R/BgDhdycOHEC8fHxHvU9fvy4x32pfeAuD7np2bMn+vbtC03TGuyjaRr69u3LMKE6GChUx/Tp06HX6xt83mAwYMaMGV6siNoK7vJQHYWFhYiKikJDbw1N03D+/HlERUV5uTLyd9xCoTruv/9+JCYmQqer+/bQ6XRITExkmFC9GChUr2nTptV7HEXTNEyfPt0HFVFbwF0eqte1a9cQERGB6upqt3a9Xo/Lly8jNDTUR5WRP+MWCtWrU6dOGDVqFAwGg6tNr9dj1KhRDBNqEAOFGpSamoo7d+64/hYRTJs2zYcVkb/jLg81qKKiAmFhYbh16xYAIDAwEFeuXEGHDh18XBn5K26hUIOsVivGjh0Lo9EIg8GA5557jmFCjWKgUKOmTp2K6upq1NTUYMqUKb4uh/ycoeku6uzfvx8XLlzw5pDUQjU1NTCZTBAR3Lx5E1lZWb4uiZohOjoajz32mPcGFC9KTk4WAHzwwYeXHsnJyd78iItXt1AAIDk5Gdu2bfP2sHQXJkyYAACYP38+NE3D8OHDfVsQNUvt+vMmrwcKtT1PPPGEr0ugNoKBQk2q7zc9RPXhO4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyrTbQBk4cCD0ej0efvhh5dOePXs2OnbsCE3T8NVXXzW73x//+EfYbDbs3LlTeW2tKScnB7GxsdA0rcFH165dlYzF9eef2m2gHDx4EE8++WSrTHvTpk1455137rqftNHrhiclJaGgoABxcXGw2WwQEYgIqqur4XA4cPnyZVgsFiVjcf35p3Z/+YL67o7na8888wzKysp8XYYyer0eZrMZZrMZPXr0UDptrj//0m63UGoZjcZWma6nb3RvfCBEBNu2bcPGjRtbfaymbN++Xen0uP78i98HSk1NDVasWIGYmBiYzWb069cPdrsdAJCRkQGr1QqdTocBAwYgIiICRqMRVqsV/fv3x9ChQxEdHQ2TyYTg4GAsXbq0zvRPnz6N+Ph4WK1WmM1mDB06FJ9++qnHNQDfrfA1a9agZ8+eCAwMhM1mw5IlS+qM5Um/Tz/9FDExMdA0Db/5zW8AABs2bIDVaoXFYsGOHTswZswYBAUFISoqCpmZmXVqffXVV9GzZ0+YzWaEhYWhW7duePXVVzFx4sS7WwmthOuvba+/ennzArbJycnNvmju4sWLJTAwULKzs6W0tFReeeUV0el0cvDgQRER+elPfyoA5PPPP5eKigq5cuWKjB49WgDIH/7wBykpKZGKigpZsGCBAJCvvvrKNe2RI0dKbGysfPvtt+J0OuWbb76RRx99VEwmk5w8edLjGpYtWyaapsm6deuktLRUHA6HrF+/XgBIXl6eazqe9rtw4YIAkDfffNPttQDko48+krKyMikuLpahQ4eK1WqVqqoqV79Vq1aJXq+XHTt2iMPhkEOHDklERIQMHz68Wctd5O7Wl4hIXFyc2Gw2t7af/OQn8vXXX9fpy/Xnf+uvJfw6UCorK8VisUhKSoqrzeFwSGBgoMyfP19E/v6GvHHjhqvPu+++KwDc3sB//etfBYBs3brV1TZy5Eh56KGH3MY8cuSIAJDFixd7VIPD4RCLxSKjRo1ym05mZqbbG83TfiKNvyErKytdbbVv5tOnT7vaBg4cKIMGDXIb49/+7d9Ep9PJ7du3pTlaEiio5wrsjQUK1993/GH9tYRf7/KcOHECDocDffr0cbWZzWZERkYiPz+/wdcFBAQAAKqrq11ttfvaTqez0TH79u0Lm82GI0eOeFTD6dOn4XA4MHLkyEan62m/5qidz3+cp1u3btU5y1BTUwOj0Qi9Xq9s7Kb841keEcFPfvITj1/L9ef79Xe3/DpQKioqAADLly93+y7DuXPn4HA4Wm1co9HoWslN1XDx4kUAQHh4eKPT9LRfSz399NM4dOgQduzYgcrKSnzxxRfYvn07nn32WZ++ITMyMtw+1K2J6893/DpQaldeenq62387EcH+/ftbZczq6mpcu3YNMTExHtVgMpkAALdv3250up72a6mVK1dixIgRmDlzJoKCgjB+/HhMnDjRo+9V3Au4/nzLrwOl9gh/Y99WVG3Pnj24c+cO+vfv71ENffr0gU6nQ25ubqPT9bRfSx09ehRnzpxBSUkJnE4nzp8/jw0bNiAkJKRVx/XUpUuXMGvWrFabPtefb/l1oJhMJsyaNQuZmZnYsGEDysvLUVNTg4sXL+LSpUtKxqiqqkJZWRmqq6vx5ZdfYsGCBejSpQtmzpzpUQ3h4eFISkpCdnY2Nm/ejPLychw5cqTOdwY87ddSzz//PGJiYnDz5k2l020pEUFlZSVycnIQFBSkbLpcf37Gm0eA7+ao8+3btyUtLU1iYmLEYDBIeHi4JCUlydGjRyUjI0MsFosAkK5du8revXtl9erVYrPZBIBERETIe++9J1u3bpWIiAgBICEhIZKZmSkiIlu2bJEnn3xSOnfuLAaDQUJDQ2Xy5Mly7tw5j2sQEblx44bMnj1bQkNDpUOHDjJkyBBZsWKFAJCoqCg5fPiwx/3efPNNiYyMFABisVhk7Nixsn79etd8Pvjgg3LmzBnZuHGjBAUFCQDp0qWL6zTp7t27JTQ01O3sitFolF69eklOTk6rrq/333+/wTM8//hYvny5iAjXn5+tPxU0Ee/98KD2Xqu8t3Hr2bBhA06dOoX09HRXW1VVFV566SVs2LABpaWlMJvNHk2L68v72vr6a/e/5bmXFBUVYcGCBXWOFwQEBCAmJgZOpxNOp9PjNyR5172w/vz6GAo1j9lshtFoxObNm3H58mU4nU4UFhZi06ZNWLFiBVJSUpQevyC17oX1x0C5h9hsNuzatQvffPMNevToAbPZjISEBGzZsgWrV6/Gu+++6+sSqRH3wvrjLs89ZujQofjLX/7i6zLoLrX19cctFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImW8/mvjixcvIisry9vD0l2ovXUE11fbdPHiRURFRXl3UG9ebzI5ObnJ643ywQcf6h739DVlqW2qvUk3t1SoKTyGQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyhh8XQD5l9zcXBw4cMCtLT8/HwDw+uuvu7UPHjwYTzzxhNdqI/+niYj4ugjyH3/5y1/w1FNPwWg0QqerfwP2zp07cDqd2LVrF0aNGuXlCsmfMVDITU1NDSIiInD16tVG+4WEhKC4uBgGAzdy6e94DIXc6PV6TJ06FQEBAQ32CQgIwLRp0xgmVAcDheqYPHkyqqqqGny+qqoKkydP9mJF1FZwl4fq1aVLF5w/f77e56KionD+/HlomublqsjfcQuF6pWamgqj0VinPSAgADNmzGCYUL24hUL1On78OBISEup97uuvv0afPn28XBG1BQwUalBCQgKOHz/u1hYfH1+njagWd3moQdOnT3fb7TEajZgxY4YPKyJ/xy0UatD58+fRtWtX1L5FNE1DQUEBunbt6tvCyG9xC4UaFBMTg0ceeQQ6nQ6apmHgwIEME2oUA4UaNX36dOh0Ouj1ekybNs3X5ZCf4y4PNaqkpAT33XcfAOBvf/sbIiIifFwR+TMGShOysrIwadIkX5dBfsBut2PixIm+LsOv8ccYHrLb7b4uQbn09HQAwAsvvNBov9zcXGiahmHDhnmjLL/EfyqeYaB46F78z7Rt2zYATc/b6NGjAQBBQUGtXpO/YqB4hoFCTWrPQULNw7M8RKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQFFu7di06d+4MTdPw9ttv+7ocpXJychAbGwtN06BpGiIjI5Gamtrk6w4fPoyUlBR069YNgYGBCAsLw0MPPYRf/OIXrj4pKSmu6Tb1+PDDD+vU8p//+Z+N1vDGG29A0zTodDrEx8fjk08+afHyoLoYKIotXrwY+/bt83UZrSIpKQkFBQWIi4uDzWZDUVERfve73zX6mq+//hqJiYmIjIzEnj17UFZWhn379mH06NH4+OOP3fru2rUL169fh9PpxKVLlwAAY8eORVVVFSoqKlBcXIwf//jHdWoBgE2bNsHpdNZbQ01NDX79618DAEaMGIH8/Px2fbGo1sRA8QOVlZVITEz0dRmtYu3atQgODkZGRga6du0Kk8mEHj164Oc//znMZrOrn6ZpePzxx2Gz2WAwGNzajUYjLBYLwsPDMWDAgDpjDBgwAEVFRdi+fXu9NeTk5OCBBx5QP3NUBwPFD2zevBnFxcW+LqNVXL16FWVlZbh27Zpbe0BAAHbu3On6OzMzExaLpcnpzZkzB88++6xb2/z58wEAb731Vr2veeONN/Diiy82t3S6CwwUL8nNzcWgQYNgsVgQFBSEvn37ory8HAsXLsSLL76IM2fOQNM0dO/eHRkZGbBardDpdBgwYAAiIiJgNBphtVrRv39/DB06FNHR0TCZTAgODsbSpUt9PXsNGjhwICoqKjBixAh89tlnrTLGiBEj0KtXL+zZswcnTpxwe+6zzz6Dw+HAU0891SpjkzsGihdUVFRg7NixSE5OxrVr13Dq1Cn06NEDVVVVyMjIwPe//33ExcVBRHD69GksXLgQS5YsgYjgrbfewrfffouioiIMGzYMeXl5ePnll5GXl4dr165hxowZWLNmDQ4fPuzr2azX0qVL8cgjj+Dw4cMYMmQIevfujV/+8pd1tlhaau7cuQBQ50D4unXrsGjRIqVjUcMYKF5w9uxZlJeXo3fv3jCZTIiIiEBOTg7CwsKafG1CQgIsFgtCQ0MxefJkAN/d0S8sLAwWi8V1liU/P79V5+Fumc1m7Nu3D7/61a8QHx+PY8eOIS0tDb169UJubq6ycWbMmAGr1Yp3330XlZWVAICCggIcPHgQU6ZMUTYONY6B4gWxsbHo3LkzUlNTsXLlSpw9e/auphMQEAAAqK6udrXV3sy8oTMc/sBoNGLBggU4fvw4Dhw4gOeeew7FxcWYMGECSktLlYxhs9kwZcoUlJaWYuvWrQC+u03I/PnzXcuNWh8DxQvMZjN2796NIUOGYNWqVYiNjUVKSorrP2l78uijj+L3v/895s2bh5KSEuzZs0fZtGsPzr799tu4fv06tm3b5toVIu9goHhJ7969sXPnThQWFiItLQ12ux1r1671dVnKffLJJ64biAHffV/kH7eoatXeJ9nhcCgb++GHH8bgwYPx17/+FXPmzMGECRMQEhKibPrUNAaKFxQWFuLYsWMAgPDwcLz22mvo37+/q+1ecujQIVitVtfft2/frnc+a8/G9OvXT+n4tVsp2dnZTd4RkdRjoHhBYWEh5s6di/z8fFRVVSEvLw/nzp3D4MGDAQCdOnVCYWEhzp49ixs3bvj18ZCGOJ1OXL58GR9//LFboADAuHHjkJWVhevXr6OsrAw7duzASy+9hB/84AfKA2XixIkICwvDuHHjEBsbq3Ta5AGhRtntdmnOYlq3bp1EREQIALFarTJ+/Hg5e/asJCYmSkhIiOj1ern//vtl2bJlUl1dLSIiX375pXTp0kXMZrMMGTJEXn75ZbFYLAJAunbtKnv37pXVq1eLzWYTABIRESHvvfeebN261TVWSEiIZGZmNmvekpOTJTk52eP+77//vsTFxQmARh/vv/++6zW7du2SSZMmSVxcnAQGBkpAQID07NlTVq5cKbdu3aozRnl5uQwbNkw6deokAESn00n37t1l1apVDdYSFhYmzz//vOu5pUuXyr59+1x/L1++XCIjI13TS0hIkL179zZnUQkAsdvtzXpNe6SJiHg9xdqQrKwsTJo0CffiYpowYQKAv9/jmBqmaRrsdvs9eY9rlbjLQ0TKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMoYmu5CwHdX7LpX3cvzRt7FS0A24eLFi9i3b5+vy/Cp2ttitPeryCcmJiIqKsrXZfg1Bgo1qfY6qllZWT6uhEW5gXgAABnPSURBVPwdj6EQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlDH4ugDyL1euXEF5eblbW0VFBQCgoKDArT0oKAhhYWFeq438nyYi4usiyH9s3rwZs2fP9qjvpk2b8KMf/aiVK6K2hIFCbkpLSxEREQGn09loP6PRiMuXLyMkJMRLlVFbwGMo5CYkJASjR4+GwdDw3rDBYMCYMWMYJlQHA4XqSE1NRU1NTYPP19TUIDU11YsVUVvBXR6q49atWwgNDYXD4aj3ebPZjCtXrsBisXi5MvJ33EKhOkwmE8aNGwej0VjnOaPRiKSkJIYJ1YuBQvWaMmVKvQdmnU4npkyZ4oOKqC3gLg/Vq7q6Gp07d0Zpaalbe3BwMIqLi+vdeiHiFgrVy2AwICUlBQEBAa42o9GIKVOmMEyoQQwUatDkyZNRVVXl+tvpdGLy5Mk+rIj8HXd5qEEigqioKBQWFgIAIiMjUVhYCE3TfFwZ+StuoVCDNE1DamoqAgICYDQaMX36dIYJNYqBQo2q3e3h2R3yRLv9tfH+/fvxxhtv+LqMNqFDhw4AgF/84hc+rqRtWLRoER577DFfl+ET7XYL5cKFC8jOzvZ1GW2CTqeDTtdu3yrNkp2djQsXLvi6DJ9pt1sotbZt2+brEvzemDFjAHBZeaK9H2Nq94FCTavd5SFqCrdjiUgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgeGjt2rXo3LkzNE3D22+/7etyPHLnzh2kp6cjMTHRq+Pm5OQgNjYWmqZB0zRERkZ6dOvSw4cPIyUlBd26dUNgYCDCwsLw0EMPuV3YKSUlxTXdph4ffvhhnVr+8z//s9Ea3njjDWiaBp1Oh/j4eHzyySctXh7tCQPFQ4sXL8a+fft8XYbHTp06hWHDhmHRokUN3lK0tSQlJaGgoABxcXGw2WwoKirC7373u0Zf8/XXXyMxMRGRkZHYs2cPysrKsG/fPowePRoff/yxW99du3bh+vXrcDqduHTpEgBg7NixqKqqQkVFBYqLi/HjH/+4Ti0AsGnTpnpvYAZ8d8/mX//61wCAESNGID8/H8OGDWvJomh3GCitqLKy0utbB8B3/+lfeuklzJs3Dw8//LDXx78ba9euRXBwMDIyMtC1a1eYTCb06NEDP//5z2E2m139NE3D448/DpvNBoPB4NZuNBphsVgQHh6OAQMG1BljwIABKCoqwvbt2+utIScnBw888ID6mWtHGCitaPPmzSguLvb6uA899BBycnIwdepUBAYGen38u3H16lWUlZXh2rVrbu0BAQHYuXOn6+/MzEyP7qs8Z84cPPvss25t8+fPBwC89dZb9b7mjTfewIsvvtjc0ukfMFBaKDc3F4MGDYLFYkFQUBD69u2L8vJyLFy4EC+++CLOnDkDTdPQvXt3ZGRkwGq1QqfTYcCAAYiIiIDRaITVakX//v0xdOhQREdHw2QyITg4GEuXLvX17HnNwIEDUVFRgREjRuCzzz5rlTFGjBiBXr16Yc+ePThx4oTbc5999hkcDgeeeuqpVhm7vWCgtEBFRQXGjh2L5ORkXLt2DadOnUKPHj1QVVWFjIwMfP/730dcXBxEBKdPn8bChQuxZMkSiAjeeustfPvttygqKsKwYcOQl5eHl19+GXl5ebh27RpmzJiBNWvW4PDhw76eTa9YunQpHnnkERw+fBhDhgxB79698ctf/rLOFktLzZ07FwDqHFhft24dFi1apHSs9oiB0gJnz55FeXk5evfuDZPJhIiICOTk5CAsLKzJ1yYkJMBisSA0NNR1e8+YmBiEhYXBYrG4zork5+e36jz4C7PZjH379uFXv/oV4uPjcezYMaSlpaFXr17Izc1VNs6MGTNgtVrx7rvvorKyEgBQUFCAgwcP8r5DCjBQWiA2NhadO3dGamoqVq5cibNnz97VdGpvSF5dXe1qq70heUNnJO5FRqMRCxYswPHjx3HgwAE899xzKC4uxoQJE1BaWqpkDJvNhilTpqC0tBRbt24FAKSnp2P+/PluN4anu8NAaQGz2Yzdu3djyJAhWLVqFWJjY5GSkuL6z0d379FHH8Xvf/97zJs3DyUlJdizZ4+yadcenH377bdx/fp1bNu2zbUrRC3DQGmh3r17Y+fOnSgsLERaWhrsdjvWrl3r67L83ieffIL09HTX30lJSW5baLWmTZsGAEq/S/Pwww9j8ODB+Otf/4o5c+ZgwoQJCAkJUTb99oyB0gKFhYU4duwYACA8PByvvfYa+vfv72qjhh06dAhWq9X19+3bt+tdbrVnY/r166d0/NqtlOzsbLzwwgtKp92eMVBaoLCwEHPnzkV+fj6qqqqQl5eHc+fOYfDgwQCATp06obCwEGfPnsWNGzfa1fGQhjidTly+fBkff/yxW6AAwLhx45CVlYXr16+jrKwMO3bswEsvvYQf/OAHygNl4sSJCAsLw7hx4xAbG6t02u2atFN2u12aM/vr1q2TiIgIASBWq1XGjx8vZ8+elcTERAkJCRG9Xi/333+/LFu2TKqrq0VE5Msvv5QuXbqI2WyWIUOGyMsvvywWi0UASNeuXWXv3r2yevVqsdlsAkAiIiLkvffek61bt7rGCgkJkczMzGbN2/79++Xxxx+X++67TwAIAImMjJTExETJzc1t1rRERJKTkyU5Odnj/u+//77ExcW5xm7o8f7777tes2vXLpk0aZLExcVJYGCgBAQESM+ePWXlypVy69atOmOUl5fLsGHDpFOnTgJAdDqddO/eXVatWtVgLWFhYfL888+7nlu6dKns27fP9ffy5cslMjLSNb2EhATZu3dvcxaVABC73d6s19xLNBERL2eYX8jKysKkSZPQTme/WSZMmACA9zb2hKZpsNvtmDhxoq9L8Qnu8hCRMgyUNiA/P9+jn+unpKT4ulRq5wxNdyFfi4+P564ZtQncQiEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyrT7yxfUXo2MGnbgwAEAXFbUtHYbKNHR0UhOTvZ1GW2CwdBu3ybNlpycjOjoaF+X4TPt9pqy5Lna66NmZWX5uBLydzyGQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEymgiIr4ugvzHb3/7W2RkZKCmpsbVVlJSAgAIDw93ten1eixcuBAzZ870donkxxgo5ObEiROIj4/3qO/x48c97kvtA3d5yE3Pnj3Rt29faJrWYB9N09C3b1+GCdXBQKE6pk+fDr1e3+DzBoMBM2bM8GJF1FZwl4fqKCwsRFRUFBp6a2iahvPnzyMqKsrLlZG/4xYK1XH//fcjMTEROl3dt4dOp0NiYiLDhOrFQKF6TZs2rd7jKJqmYfr06T6oiNoC7vJQva5du4aIiAhUV1e7tev1ely+fBmhoaE+qoz8GbdQqF6dOnXCqFGjYDAYXG16vR6jRo1imFCDGCjUoNTUVNy5c8f1t4hg2rRpPqyI/B13eahBFRUVCAsLw61btwAAgYGBuHLlCjp06ODjyshfcQuFGmS1WjF27FgYjUYYDAY899xzDBNqFAOFGjV16lRUV1ejpqYGU6ZM8XU55OcMTXdpn7Kysnxdgl+oqamByWSCiODmzZtcLv/fxIkTfV2CX+IxlAY09lsWIn5s6sddnkbY7XaISLt9JCcnIzk5Gbt378aePXt8Xo8/POx2u6/fln6NuzzUpCeeeMLXJVAbwUChJtX3mx6i+vCdQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGCitZPbs2ejYsSM0TcNXX33l63K8IicnB7GxsdA0ze0REBCAzp07Y/jw4VizZg1KS0t9XSq1EgZKK9m0aRPeeecdX5fhVUlJSSgoKEBcXBxsNhtEBHfu3EFxcTGysrLQrVs3pKWloXfv3vjiiy98XS61AgYKtSpN0xAcHIzhw4djy5YtyMrKwuXLl/HMM8+grKzM1+WRYgyUVsTLSNaVnJyMmTNnori4GG+//bavyyHFGCiKiAjWrFmDnj17IjAwEDabDUuWLKnTr6amBitWrEBMTAzMZjP69evnuqzghg0bYLVaYbFYsGPHDowZMwZBQUGIiopCZmam23Ryc3MxaNAgWCwWBAUFoW/fvigvL29yDH8wc+ZMAMCf/vQnVxuXyz1CqF4AxG63e9x/2bJlommarFu3TkpLS8XhcMj69esFgOTl5bn6LV68WAIDAyU7O1tKS0vllVdeEZ1OJwcPHnRNB4B89NFHUlZWJsXFxTJ06FCxWq1SVVUlIiI3b96UoKAgef3116WyslKKiopk/PjxUlJS4tEYnkpOTpbk5ORmvUZEJC4uTmw2W4PPl5eXCwCJjo52tbWV5WK324Ufm4ZxyTSgOYHicDjEYrHIqFGj3NozMzPdAqWyslIsFoukpKS4vTYwMFDmz58vIn//4FRWVrr61AbT6dOnRUTkm2++EQDy4Ycf1qnFkzE81VqBIiKiaZoEBwd7XLO/LBcGSuO4y6PA6dOn4XA4MHLkyEb7nThxAg6HA3369HG1mc1mREZGIj8/v8HXBQQEAACcTicAIDY2Fp07d0ZqaipWrlyJs2fPtngMb6qoqICIICgoCACXy72EgaLAxYsXAQDh4eGN9quoqAAALF++3O17GufOnYPD4fB4PLPZjN27d2PIkCFYtWoVYmNjkZKSgsrKSmVjtKaTJ08CAOLj4wFwudxLGCgKmEwmAMDt27cb7VcbOOnp6XXu97J///5mjdm7d2/s3LkThYWFSEtLg91ux9q1a5WO0Vr+/Oc/AwDGjBkDgMvlXsJAUaBPnz7Q6XTIzc1ttF90dDRMJlOLvzlbWFiIY8eOAfjuw/jaa6+hf//+OHbsmLIxWktRURHS09MRFRWFH/7whwC4XO4lDBQFwsPDkZSUhOzsbGzevBnl5eU4cuQINm7c6NbPZDJh1qxZyMzMxIYNG1BeXo6amhpcvHgRly5d8ni8wsJCzJ07F/n5+aiqqkJeXh7OnTuHwYMHKxujpUS+uxfynTt3ICIoKSmB3W7H448/Dr1ej+3bt7uOobSn5XLP8/JB4DYDzTxtfOPGDZk9e7aEhoZKhw4dZMiQIbJixQoBIFFRUXL48GEREbl9+7akpaVJTEyMGAwGCQ8Pl6SkJDl69KisX79eLBaLAJAHH3xQzpw5Ixs3bpSgoCABIF26dJGTJ0/K2bNnJTExUUJCQkSv18v9998vy5Ytk+rq6ibHaI7mnuX54IMPpF+/fmKxWCQgIEB0Op0AcJ3RGTRokPzsZz+Tq1ev1nltW1kuPMvTON4svQGapsFut2PixIm+LsVnJkyYAADYtm2bjyvxH1lZWZg0aRL4sakfd3mISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGYOvC/Bn7f1q6LW3B8nKyvJxJf6jvb8nmsJLQDaANzqnxvBjUz9uoTSAb5i/q72uLrdUqCk8hkJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMoYfF0A+Zfc3FwcOHDArS0/Px8A8Prrr7u1Dx48GE888YTXaiP/p4mI+LoI8h9/+ctf8NRTT8FoNEKnq38D9s6dO3A6ndi1axdGjRrl5QrJnzFQyE1NTQ0iIiJw9erVRvuFhISguLgYBgM3cunveAyF3Oj1ekydOhUBAQEN9gkICMC0adMYJlQHA4XqmDx5Mqqqqhp8vqqqCpMnT/ZiRdRWcJeH6tWlSxecP3++3ueioqJw/vx5aJrm5arI33ELheqVmpoKo9FYpz0gIAAzZsxgmFC9uIVC9Tp+/DgSEhLqfe7rr79Gnz59vFwRtQUMFGpQQkICjh8/7tYWHx9fp42oFnd5qEHTp0932+0xGo2YMWOGDysif8ctFGrQ+fPn0bVrV9S+RTRNQ0FBAbp27erbwshvcQuFGhQTE4NHHnkEOp0OmqZh4MCBDBNqFAOFGjV9+nTodDro9XpMmzbN1+WQn+MuDzWqpKQE9913HwDgb3/7GyIiInxcEfmzdhco/P4EeVM7+3i1z8sXLFy4EI899pivy2gzcnNzoWkahg0bVu/z6enpAIAXXnjBm2X5tf379yMjI8PXZXhduwyUxx57DBMnTvR1GW3G6NGjAQBBQUH1Pr9t2zYA4DL9JwwUono0FCRE/4xneYhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoDTT7Nmz0bFjR2iahq+++srX5bTInTt3kJ6ejsTERK+Om5OTg9jYWGia5vYICAhA586dMXz4cKxZswalpaVerYtajoHSTJs2bcI777zj6zJa7NSpUxg2bBgWLVoEh8Ph1bGTkpJQUFCAuLg42Gw2iAju3LmD4uJiZGVloVu3bkhLS0Pv3r3xxRdfeLU2ahkGSjt0+PBhvPTSS5g3bx4efvhhX5cD4LtLcwYHB2P48OHYsmULsrKycPnyZTzzzDMoKyvzdXnkIQbKXWjr16V96KGHkJOTg6lTpyIwMNDX5dQrOTkZM2fORHFxMd5++21fl0MeYqA0QUSwZs0a9OzZE4GBgbDZbFiyZEmdfjU1NVixYgViYmJgNpvRr18/2O12AMCGDRtgtVphsViwY8cOjBkzBkFBQYiKikJmZqbbdHJzczFo0CBYLBYEBQWhb9++KC8vb3KMe9HMmTMBAH/6059cbVzOfk7aGQBit9s97r9s2TLRNE3WrVsnpaWl4nA4ZP369QJA8vLyXP0WL14sgYGBkp2dLaWlpfLKK6+ITqeTgwcPuqYDQD766CMpKyuT4uJiGTp0qFitVqmqqhIRkZs3b0pQUJC8/vrrUllZKUVFRTJ+/HgpKSnxaIy78eijj8pDDz10168XEUlOTpbk5ORmvy4uLk5sNluDz5eXlwsAiY6OdrW1leVst9ulHX68pN3NcXMCxeFwiMVikVGjRrm1Z2ZmugVKZWWlWCwWSUlJcXttYGCgzJ8/X0T+/kavrKx09akNptOnT4uIyDfffCMA5MMPP6xTiydj3A1/DhQREU3TJDg4WETa1nJur4HCXZ5GnD59Gg6HAyNHjmy034kTJ+BwONCnTx9Xm9lsRmRkJPLz8xt8XUBAAADA6XQCAGJjY9G5c2ekpqZi5cqVOHv2bIvHaMsqKiogIq6LZHM5+z8GSiMuXrwIAAgPD2+0X0VFBQBg+fLlbt+rOHfuXLNOyZrNZuzevRtDhgzBqlWrEBsbi5SUFFRWVioboy05efIkACA+Ph4Al3NbwEBphMlkAgDcvn270X61gZOeng75bjfS9di/f3+zxuzduzd27tyJwsJCpKWlwW63Y+3atUrHaCv+/Oc/AwDGjBkDgMu5LWCgNKJPnz7Q6XTIzc1ttF90dDRMJlOLvzlbWFiIY8eOAfjuw/Paa6+hf//+OHbsmLIx2oqioiKkp6cjKioKP/zhDwFwObcFDJRGhIeHIykpCdnZ2di8eTPKy8tx5MgRbNy40a2fyWTCrFmzkJmZiQ0bNqC8vBw1NTW4ePEiLl265PF4hYWFmDt3LvLz81FVVYW8vDycO3cOgwcPVjaGvxER3Lx5E3fu3IGIoKSkBHa7HY8//jj0ej22b9/uOobC5dwGePkgsM+hmaeNb9y4IbNnz5bQ0FDp0KGDDBkyRFasWCEAJCoqSg4fPiwiIrdv35a0tDSJiYkRg8Eg4eHhkpSUJEePHpX169eLxWIRAPLggw/KmTNnZOPGjRIUFCQApEuXLnLy5Ek5e/asJCYmSkhIiOj1ern//vtl2bJlUl1d3eQYzbF//355/PHH5b777hMAAkAiIyMlMTFRcnNzmzUtkeaf5fnggw+kX79+YrFYJCAgQHQ6nQBwndEZNGiQ/OxnP5OrV6/WeW1bWc7t9SyPJtK+bg+vaRrsdjvvw6vQhAkTAPz9HscEZGVlYdKkSWhnHy/u8hCROgyUe0B+fn6dSwHU90hJSfF1qXSPM/i6AGq5+Pj4drdpTf6JWyhEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISJl2ecU2Im9pZx+v9nc9FN6jlqj1tLstFCJqPTyGQkTKMFCISBkGChEpYwDAm6kQkRL/D6Y3XMJuc/s1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Training**"
      ],
      "metadata": {
        "id": "wIXnB3LYsE6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint #this will help to save model for future use\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model_weigths.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001)) \n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RzqzcmTn00X",
        "outputId": "d53c49a3-13a2-42c6-90c2-2f7fafd86e67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 6.0225\n",
            "Epoch 1: loss improved from inf to 5.99776, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 4s 46ms/step - loss: 5.9978\n",
            "Epoch 2/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 5.6255\n",
            "Epoch 2: loss improved from 5.99776 to 5.65125, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 5.6512\n",
            "Epoch 3/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 5.5289\n",
            "Epoch 3: loss improved from 5.65125 to 5.53966, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 5.5397\n",
            "Epoch 4/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 5.4758\n",
            "Epoch 4: loss improved from 5.53966 to 5.47814, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 5.4781\n",
            "Epoch 5/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 5.3700\n",
            "Epoch 5: loss improved from 5.47814 to 5.40522, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.4052\n",
            "Epoch 6/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 5.3002\n",
            "Epoch 6: loss improved from 5.40522 to 5.33374, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.3337\n",
            "Epoch 7/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 5.1755\n",
            "Epoch 7: loss improved from 5.33374 to 5.22261, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2226\n",
            "Epoch 8/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 5.1088\n",
            "Epoch 8: loss improved from 5.22261 to 5.13801, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 5.1380\n",
            "Epoch 9/100\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 5.0671\n",
            "Epoch 9: loss improved from 5.13801 to 5.07700, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 5.0770\n",
            "Epoch 10/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 5.0354\n",
            "Epoch 10: loss improved from 5.07700 to 5.02937, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 5.0294\n",
            "Epoch 11/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.9500\n",
            "Epoch 11: loss improved from 5.02937 to 4.96219, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 4.9622\n",
            "Epoch 12/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.9061\n",
            "Epoch 12: loss improved from 4.96219 to 4.90881, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 4.9088\n",
            "Epoch 13/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 4.8139\n",
            "Epoch 13: loss improved from 4.90881 to 4.84916, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 4.8492\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.7933\n",
            "Epoch 14: loss improved from 4.84916 to 4.79333, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.7933\n",
            "Epoch 15/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 4.6852\n",
            "Epoch 15: loss improved from 4.79333 to 4.72486, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.7249\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.6807\n",
            "Epoch 16: loss improved from 4.72486 to 4.68070, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 4.6807\n",
            "Epoch 17/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.6311\n",
            "Epoch 17: loss improved from 4.68070 to 4.64071, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.6407\n",
            "Epoch 18/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 4.4913\n",
            "Epoch 18: loss improved from 4.64071 to 4.52116, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 4.5212\n",
            "Epoch 19/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.4194\n",
            "Epoch 19: loss improved from 4.52116 to 4.42063, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 4.4206\n",
            "Epoch 20/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.2856\n",
            "Epoch 20: loss improved from 4.42063 to 4.32237, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 4.3224\n",
            "Epoch 21/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.1501\n",
            "Epoch 21: loss improved from 4.32237 to 4.19460, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 4.1946\n",
            "Epoch 22/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 4.0380\n",
            "Epoch 22: loss improved from 4.19460 to 4.05671, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 4.0567\n",
            "Epoch 23/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 3.8850\n",
            "Epoch 23: loss improved from 4.05671 to 3.96659, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 3.9666\n",
            "Epoch 24/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.8965\n",
            "Epoch 24: loss improved from 3.96659 to 3.92384, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 3.9238\n",
            "Epoch 25/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 3.7585\n",
            "Epoch 25: loss improved from 3.92384 to 3.82491, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 3.8249\n",
            "Epoch 26/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.7053\n",
            "Epoch 26: loss improved from 3.82491 to 3.71984, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 3.7198\n",
            "Epoch 27/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.5759\n",
            "Epoch 27: loss improved from 3.71984 to 3.57227, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 3.5723\n",
            "Epoch 28/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.4406\n",
            "Epoch 28: loss improved from 3.57227 to 3.47362, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 3.4736\n",
            "Epoch 29/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.4547\n",
            "Epoch 29: loss improved from 3.47362 to 3.45037, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 3.4504\n",
            "Epoch 30/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 3.3492\n",
            "Epoch 30: loss improved from 3.45037 to 3.36326, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 3.3633\n",
            "Epoch 31/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.2570\n",
            "Epoch 31: loss improved from 3.36326 to 3.29172, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 3.2917\n",
            "Epoch 32/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.2188\n",
            "Epoch 32: loss improved from 3.29172 to 3.23807, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 3.2381\n",
            "Epoch 33/100\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.1396\n",
            "Epoch 33: loss improved from 3.23807 to 3.16634, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 3.1663\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.0926\n",
            "Epoch 34: loss improved from 3.16634 to 3.09261, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 49ms/step - loss: 3.0926\n",
            "Epoch 35/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 2.8339\n",
            "Epoch 35: loss improved from 3.09261 to 2.93834, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 59ms/step - loss: 2.9383\n",
            "Epoch 36/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.8225\n",
            "Epoch 36: loss improved from 2.93834 to 2.87014, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 2.8701\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.7644\n",
            "Epoch 37: loss improved from 2.87014 to 2.76438, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 2.7644\n",
            "Epoch 38/100\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 2.4343\n",
            "Epoch 38: loss improved from 2.76438 to 2.47157, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 2.4716\n",
            "Epoch 39/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.3087\n",
            "Epoch 39: loss improved from 2.47157 to 2.37207, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 2.3721\n",
            "Epoch 40/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.2199\n",
            "Epoch 40: loss improved from 2.37207 to 2.23589, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 2.2359\n",
            "Epoch 41/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.0255\n",
            "Epoch 41: loss improved from 2.23589 to 2.05158, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 2.0516\n",
            "Epoch 42/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.8054\n",
            "Epoch 42: loss improved from 2.05158 to 1.83838, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 60ms/step - loss: 1.8384\n",
            "Epoch 43/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.8022\n",
            "Epoch 43: loss did not improve from 1.83838\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.8554\n",
            "Epoch 44/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.6887\n",
            "Epoch 44: loss improved from 1.83838 to 1.77247, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.7725\n",
            "Epoch 45/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.4878\n",
            "Epoch 45: loss improved from 1.77247 to 1.52030, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 1.5203\n",
            "Epoch 46/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.3084\n",
            "Epoch 46: loss improved from 1.52030 to 1.38510, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 1.3851\n",
            "Epoch 47/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 1.2423\n",
            "Epoch 47: loss improved from 1.38510 to 1.32703, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 1.3270\n",
            "Epoch 48/100\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.2921\n",
            "Epoch 48: loss improved from 1.32703 to 1.27956, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 1.2796\n",
            "Epoch 49/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 1.1370\n",
            "Epoch 49: loss improved from 1.27956 to 1.15491, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 1.1549\n",
            "Epoch 50/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.9725\n",
            "Epoch 50: loss improved from 1.15491 to 1.03532, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 1.0353\n",
            "Epoch 51/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.8575\n",
            "Epoch 51: loss improved from 1.03532 to 0.88687, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.8869\n",
            "Epoch 52/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.8003\n",
            "Epoch 52: loss improved from 0.88687 to 0.83511, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 0.8351\n",
            "Epoch 53/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.7298\n",
            "Epoch 53: loss improved from 0.83511 to 0.77953, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 0.7795\n",
            "Epoch 54/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.7321\n",
            "Epoch 54: loss improved from 0.77953 to 0.72469, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.7247\n",
            "Epoch 55/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.6684\n",
            "Epoch 55: loss improved from 0.72469 to 0.70983, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 49ms/step - loss: 0.7098\n",
            "Epoch 56/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.6644\n",
            "Epoch 56: loss improved from 0.70983 to 0.66375, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.6638\n",
            "Epoch 57/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.6604\n",
            "Epoch 57: loss did not improve from 0.66375\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6656\n",
            "Epoch 58/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.5803\n",
            "Epoch 58: loss improved from 0.66375 to 0.57846, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.5785\n",
            "Epoch 59/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4710\n",
            "Epoch 59: loss improved from 0.57846 to 0.48579, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.4858\n",
            "Epoch 60/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3649\n",
            "Epoch 60: loss improved from 0.48579 to 0.37754, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.3775\n",
            "Epoch 61/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3028\n",
            "Epoch 61: loss improved from 0.37754 to 0.32922, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.3292\n",
            "Epoch 62/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.3439\n",
            "Epoch 62: loss did not improve from 0.32922\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3545\n",
            "Epoch 63/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3482\n",
            "Epoch 63: loss did not improve from 0.32922\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3598\n",
            "Epoch 64/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3710\n",
            "Epoch 64: loss did not improve from 0.32922\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3810\n",
            "Epoch 65/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2668\n",
            "Epoch 65: loss improved from 0.32922 to 0.28300, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2830\n",
            "Epoch 66/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.2668\n",
            "Epoch 66: loss improved from 0.28300 to 0.27516, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2752\n",
            "Epoch 67/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.2149\n",
            "Epoch 67: loss improved from 0.27516 to 0.22842, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 0.2284\n",
            "Epoch 68/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1725\n",
            "Epoch 68: loss improved from 0.22842 to 0.17776, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 59ms/step - loss: 0.1778\n",
            "Epoch 69/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.1752\n",
            "Epoch 69: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1839\n",
            "Epoch 70/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1884\n",
            "Epoch 70: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1962\n",
            "Epoch 71/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1892\n",
            "Epoch 71: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1893\n",
            "Epoch 72/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1792\n",
            "Epoch 72: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1867\n",
            "Epoch 73/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2369\n",
            "Epoch 73: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2564\n",
            "Epoch 74/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3050\n",
            "Epoch 74: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3030\n",
            "Epoch 75/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4249\n",
            "Epoch 75: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4622\n",
            "Epoch 76/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.3949\n",
            "Epoch 76: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.4456\n",
            "Epoch 77/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3048\n",
            "Epoch 77: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3278\n",
            "Epoch 78/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2839\n",
            "Epoch 78: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3088\n",
            "Epoch 79/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3726\n",
            "Epoch 79: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3774\n",
            "Epoch 80/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2955\n",
            "Epoch 80: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3009\n",
            "Epoch 81/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1888\n",
            "Epoch 81: loss did not improve from 0.17776\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1928\n",
            "Epoch 82/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1117\n",
            "Epoch 82: loss improved from 0.17776 to 0.12729, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1273\n",
            "Epoch 83/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1150\n",
            "Epoch 83: loss improved from 0.12729 to 0.11066, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1107\n",
            "Epoch 84/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1118\n",
            "Epoch 84: loss did not improve from 0.11066\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1124\n",
            "Epoch 85/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0883\n",
            "Epoch 85: loss improved from 0.11066 to 0.09290, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 57ms/step - loss: 0.0929\n",
            "Epoch 86/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0667\n",
            "Epoch 86: loss improved from 0.09290 to 0.06163, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.0616\n",
            "Epoch 87/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0341\n",
            "Epoch 87: loss improved from 0.06163 to 0.03574, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.0357\n",
            "Epoch 88/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.0204\n",
            "Epoch 88: loss improved from 0.03574 to 0.02289, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 0.0229\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0149\n",
            "Epoch 89: loss improved from 0.02289 to 0.01495, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.0149\n",
            "Epoch 90/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0126\n",
            "Epoch 90: loss improved from 0.01495 to 0.01172, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.0117\n",
            "Epoch 91/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0083\n",
            "Epoch 91: loss improved from 0.01172 to 0.00879, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.0088\n",
            "Epoch 92/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0068\n",
            "Epoch 92: loss improved from 0.00879 to 0.00723, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 0.0072\n",
            "Epoch 93/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0063\n",
            "Epoch 93: loss improved from 0.00723 to 0.00614, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.0061\n",
            "Epoch 94/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0057\n",
            "Epoch 94: loss improved from 0.00614 to 0.00552, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.0055\n",
            "Epoch 95/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.0054\n",
            "Epoch 95: loss improved from 0.00552 to 0.00522, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.0052\n",
            "Epoch 96/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.0049\n",
            "Epoch 96: loss improved from 0.00522 to 0.00498, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.0050\n",
            "Epoch 97/100\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 0.0049\n",
            "Epoch 97: loss improved from 0.00498 to 0.00463, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 0.0046\n",
            "Epoch 98/100\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.0043\n",
            "Epoch 98: loss improved from 0.00463 to 0.00434, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 98ms/step - loss: 0.0043\n",
            "Epoch 99/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0040\n",
            "Epoch 99: loss improved from 0.00434 to 0.00431, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.0043\n",
            "Epoch 100/100\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0039\n",
            "Epoch 100: loss improved from 0.00431 to 0.00430, saving model to model_weigths.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.0043\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78a01de790>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Prediction**"
      ],
      "metadata": {
        "id": "es1R0bUToWQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('model_weigths.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "N9DWXshioVzh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "  \n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-4:]\n",
        "          print(text)\n",
        "        \n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRPxyvqUooX0",
        "outputId": "d8502de5-efb6-42b6-b2c9-ee2a6b380b62"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: the stern of the\n",
            "['the', 'stern', 'of', 'the']\n",
            "1/1 [==============================] - 1s 604ms/step\n",
            "ship\n",
            "Enter your line: fuzzing up the water\n",
            "['fuzzing', 'up', 'the', 'water']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "through\n",
            "Enter your line: white all over a voyage to\n",
            "['over', 'a', 'voyage', 'to']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "greenland\n",
            "Enter your line: whales goldsmith to johnson in the\n",
            "['to', 'johnson', 'in', 'the']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "afternoon\n",
            "Enter your line: 0\n",
            "Execution completed.....\n"
          ]
        }
      ]
    }
  ]
}